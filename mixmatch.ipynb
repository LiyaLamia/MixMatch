{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaebf6f-18fd-4c74-a52f-15e178762856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progress in /home/lsals002/envs/Lamia_Feature_Extract/lib/python3.7/site-packages (1.6)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/lsals002/envs/Lamia_Feature_Extract/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboardX in /home/lsals002/envs/Lamia_Feature_Extract/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/lsals002/envs/Lamia_Feature_Extract/lib/python3.7/site-packages (from tensorboardX) (3.19.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorboardX) (1.20.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/lsals002/envs/Lamia_Feature_Extract/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "==> Preparing cifar10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "#Labeled: 500 #Unlabeled: 44500 #Val: 5000\n",
      "10000\n",
      "----------------------\n",
      "<class 'dataset.cifar10.CIFAR10_labeled'>\n",
      "==> creating WRN-28\n",
      "    Total params: 2.24M\n",
      "\n",
      "Epoch: [1 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [2 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [3 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [4 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [5 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [6 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [7 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [8 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [9 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [10 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [11 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [12 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [13 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [14 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [15 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [16 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [17 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [18 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [19 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [20 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [21 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [22 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [23 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [24 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [25 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [26 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [27 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [28 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [29 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [30 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [31 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [32 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [33 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [34 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [35 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [36 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [37 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [38 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [39 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [40 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [41 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [42 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [43 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [44 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [45 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [46 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [47 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [48 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [49 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [50 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [51 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [52 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [53 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [54 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [55 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [56 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [57 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [58 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [59 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [60 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [61 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [62 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [63 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [64 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [65 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [66 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [67 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [68 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [69 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [70 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [71 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [72 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [73 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [74 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [75 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [76 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [77 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [78 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [79 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [80 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [81 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [82 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [83 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [84 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [85 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [86 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [87 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [88 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [89 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [90 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [91 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [92 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [93 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [94 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [95 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [96 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [97 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [98 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [99 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [100 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [101 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [102 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [103 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [104 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [105 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [106 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [107 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [108 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [109 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [110 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [111 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [112 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [113 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [114 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [115 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [116 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [117 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [118 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [119 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [120 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [121 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [122 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [123 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [124 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [125 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [126 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [127 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [128 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [129 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [130 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [131 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [132 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [133 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [134 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [135 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [136 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [137 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [138 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [139 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [140 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [141 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [142 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [143 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [144 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [145 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [146 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [147 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [148 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [149 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [150 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [151 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [152 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [153 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [154 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [155 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [156 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [157 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [158 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [159 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [160 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [161 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [162 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [163 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [164 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [165 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [166 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [167 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [168 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [169 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [170 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [171 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [172 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [173 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [174 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [175 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [176 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [177 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [178 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [179 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [180 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [181 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [182 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [183 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [184 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [185 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [186 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [187 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [188 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [189 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [190 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [191 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [192 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [193 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [194 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [195 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [196 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [197 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [198 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [199 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [200 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [201 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [202 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [203 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [204 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [205 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [206 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [207 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [208 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [209 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [210 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [211 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [212 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [213 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [214 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [215 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [216 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [217 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [218 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [219 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [220 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [221 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [222 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [223 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [224 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [225 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [226 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [227 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [228 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [229 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [230 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [231 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [232 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [233 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [234 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [235 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [236 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [237 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [238 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [239 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [240 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [241 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [242 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [243 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [244 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [245 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [246 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [247 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [248 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [249 | 250] LR: 0.010000\n",
      "\n",
      "Epoch: [250 | 250] LR: 0.010000\n",
      "Best acc:\n",
      "80.12\n",
      "Mean acc:\n",
      "78.665\n",
      "-----------Test loss--------------\n",
      "[2.246165881156921, 2.1701893472671507, 2.0134509229660034, 1.7271379458904266, 1.4683353948593139, 1.324792183637619, 1.2497221744060516, 1.1985800141096115, 1.1770713639259338, 1.1508491563796996, 1.1234789496660234, 1.1244890052080154, 1.0959182149171829, 1.0976757007837294, 1.0901069194078445, 1.0806046599149703, 1.073514963388443, 1.0421124929189682, 1.0233265393972397, 1.0233801984786988, 1.0052491515874862, 0.9853528529405594, 0.9767941772937775, 0.9685244107246399, 0.9492037123441697, 0.9417586576938629, 0.9204809856414795, 0.917142135500908, 0.9166397964954376, 0.9085777091979981, 0.9154618614912033, 0.8966405022144318, 0.8907692682743072, 0.8876908802986145, 0.8753468298912048, 0.8758732223510742, 0.8685480666160583, 0.8816958051919938, 0.890297019481659, 0.8823359525203704, 0.868507428765297, 0.8629507720470428, 0.8670991730690002, 0.8795199865102767, 0.8660633188486099, 0.8535749614238739, 0.8549317044019699, 0.854352862238884, 0.8568263894319534, 0.8541401958465576, 0.8364563614130021, 0.8369648134708405, 0.8310289502143859, 0.8334210592508317, 0.8461213260889053, 0.8348941779136658, 0.8375995522737503, 0.8407305127382279, 0.8433949649333954, 0.839692251086235, 0.82446692943573, 0.8264268410205841, 0.8265707981586456, 0.8287828308343888, 0.8285783684253692, 0.8222023421525955, 0.8178004384040832, 0.82736443400383, 0.8281045335531235, 0.8292285013198852, 0.8250845777988434, 0.8103361922502518, 0.8126246756315232, 0.8106770771741867, 0.8069080168008804, 0.8099673068523408, 0.8064926707744599, 0.7936543142795562, 0.8112050622701645, 0.8168602967262268, 0.8323966693878174, 0.8137133708596229, 0.7969428512454033, 0.7845395115017891, 0.7945728766918182, 0.7934049218893051, 0.7810174241662026, 0.7943725198507309, 0.7868846204876899, 0.7816113784909249, 0.7743702325224876, 0.783901771903038, 0.7957185301184654, 0.7876661831140518, 0.7948737916350365, 0.7864765605330467, 0.7873791900277137, 0.7819330599904061, 0.7982658585906028, 0.7930317068099976, 0.7875582748651504, 0.7822104236483574, 0.7728739187121392, 0.7851755544543266, 0.7682777583599091, 0.7689531919360161, 0.7661403903365135, 0.7758769810199737, 0.7633033561706543, 0.754949152469635, 0.7644835126399994, 0.7663065937161445, 0.7574464586377144, 0.7555210128426552, 0.7580266797542572, 0.7536357614398003, 0.7631047812104225, 0.7677337327599525, 0.7685043162107468, 0.7557548490166665, 0.7505672103166581, 0.7632645711302757, 0.7587538909912109, 0.7645843708515168, 0.7546121475100517, 0.7514554163813592, 0.749115192592144, 0.7527788323163986, 0.7564357772469521, 0.7538277453184128, 0.7495475459098816, 0.7588536128401756, 0.7508324414491654, 0.753621937930584, 0.7586753734946251, 0.7479880839586258, 0.7452138051390648, 0.7455657225847244, 0.7526619094610214, 0.7540246295928955, 0.7545800453424454, 0.7467455270886422, 0.7584786820411682, 0.7641453635692597, 0.7514095678925514, 0.7530852693319321, 0.7458424913883209, 0.7575908064842224, 0.7564977586269379, 0.7488483342528344, 0.7532958897948265, 0.7471763089299202, 0.7597265121340752, 0.752599541246891, 0.7520738232135773, 0.7533516991138458, 0.7437026777863502, 0.75552674472332, 0.7486050823330879, 0.7469417715072632, 0.7452666288614274, 0.7399638429284096, 0.7424127176404, 0.7447517120838165, 0.7413611859083176, 0.7374625691771507, 0.7383574998378754, 0.7377302351593972, 0.7441261836886406, 0.7449770623445511, 0.7447251090407372, 0.7461334645748139, 0.7556175118684769, 0.7522815838456154, 0.7473332396149636, 0.7584757986664772, 0.7569108188152314, 0.7538614505529404, 0.7605351719260216, 0.7546679922938346, 0.7447684067487716, 0.7512912857532501, 0.7582295382022858, 0.7476880657672882, 0.7413667684793472, 0.7420435756444931, 0.73505063444376, 0.7396870255470276, 0.748120276927948, 0.7568716132640838, 0.7460551637411118, 0.7454286119341851, 0.741192358136177, 0.7364734768867492, 0.7429450157284737, 0.7475891721248626, 0.7508133712410927, 0.7536712205410003, 0.7451402580738068, 0.7359156993031502, 0.7423246154189109, 0.7434434926509857, 0.7347365027666092, 0.7421584188938141, 0.7413947397470474, 0.7390942618250846, 0.7353201586008072, 0.7354763701558114, 0.7321728113293647, 0.7376044806838036, 0.7357162377238273, 0.7452074205875396, 0.7547679737210273, 0.7602442100644111, 0.7554474538564682, 0.7611529213190079, 0.7621801388263703, 0.7628429991006851, 0.7602272427082062, 0.7635070195794106, 0.7540762463212013, 0.7566395950317383, 0.7559303936362266, 0.745406542122364, 0.7446114349365235, 0.7502525192499161, 0.7563476806879044, 0.7648310565948486, 0.7708961868286133, 0.760162260234356, 0.7571876928210258, 0.7570316505432129, 0.7574188467860222, 0.7596179804205895, 0.745900950729847, 0.750334330201149, 0.7487794589996338, 0.7522915089130402, 0.734837729036808, 0.7488439068198204, 0.7441548365354538, 0.7491579061746597, 0.7397295647859573, 0.7498856171965599, 0.7544158628582954, 0.7507461974024773, 0.74547547519207, 0.7380703747272491, 0.739309279024601, 0.7358646178245545]\n",
      "------------Validation loss--------\n",
      "[2.2497852277755737, 2.1814604425430297, 2.035081446170807, 1.756828100681305, 1.5003170680999756, 1.3543931722640992, 1.277039794921875, 1.2228303933143616, 1.1996301746368407, 1.172697606086731, 1.1445366728305817, 1.1440493667125702, 1.11113499045372, 1.1082426369190217, 1.0986266505718232, 1.0883377420902252, 1.0803200614452362, 1.0514748227596282, 1.0309104311466217, 1.0320174896717071, 1.0094634795188904, 0.9874397325515747, 0.9781523895263672, 0.9690704441070557, 0.9524493670463562, 0.9431548964977264, 0.9179216015338898, 0.9140184116363526, 0.909649966955185, 0.8985768866539001, 0.9041447746753692, 0.8882680690288544, 0.8806891191005707, 0.8755740916728973, 0.8631187462806702, 0.8662194275856018, 0.8574966847896576, 0.8697631227970123, 0.8774559485912323, 0.8670808017253876, 0.8512163472175598, 0.8479997599124909, 0.8540576815605163, 0.8680872905254364, 0.8599427711963653, 0.8475634765625, 0.848829699754715, 0.8476908600330353, 0.8514939486980438, 0.8487809240818024, 0.8289676785469056, 0.8235695099830628, 0.8180340027809143, 0.8244356858730316, 0.8314334309101105, 0.8191931438446045, 0.8176338100433349, 0.8188789010047912, 0.8188372659683227, 0.8228679752349853, 0.8121772265434265, 0.8119466733932496, 0.8110093343257904, 0.8121040213108063, 0.8079983431100846, 0.8033904284238815, 0.7992580306529998, 0.8058880496025086, 0.8079049503803253, 0.8127338814735413, 0.809159767627716, 0.7955629104375839, 0.7954299831390381, 0.7899522244930267, 0.7848423236608505, 0.7890164977312089, 0.7822417414188385, 0.7741698580980301, 0.7887089610099792, 0.7945842456817627, 0.8126578617095948, 0.7964606511592865, 0.7813790637254715, 0.767867448925972, 0.7767541414499283, 0.7747677284479141, 0.7643880730867386, 0.7765176546573639, 0.7687068003416061, 0.7610532885789871, 0.7560973060131073, 0.7633132165670395, 0.771546443104744, 0.7654095613956451, 0.77450124502182, 0.764835809469223, 0.7640829926729202, 0.7587597095966339, 0.773051723241806, 0.7674162065982819, 0.7612751859426499, 0.7584359914064407, 0.7508126890659332, 0.7630914944410324, 0.7485712599754334, 0.7485667216777802, 0.7456485176086426, 0.752202826142311, 0.7431511014699936, 0.736499582529068, 0.747870038151741, 0.7438365393877029, 0.737311315536499, 0.7344149297475815, 0.7381551772356033, 0.7354468840360642, 0.7425116449594498, 0.7439258140325546, 0.7426462525129318, 0.7341248524188996, 0.7295601755380631, 0.7423879861831665, 0.7384462875127792, 0.7393211823701858, 0.7310904610157013, 0.7273878514766693, 0.7267354542016983, 0.7330277580022811, 0.7380933606624603, 0.7350571501255035, 0.728851369023323, 0.7344552320241928, 0.7275906449556351, 0.7277265322208405, 0.7308965194225311, 0.7246457481384277, 0.7206240111589431, 0.7253239840269089, 0.7306520348787308, 0.7332751256227493, 0.7319758641719818, 0.720820517539978, 0.7284707152843475, 0.7357409381866455, 0.7266105622053146, 0.7277708047628403, 0.7223938351869583, 0.7309990137815475, 0.7299191498756409, 0.7226096570491791, 0.7278121185302734, 0.7225388008356094, 0.734412242770195, 0.7290954822301865, 0.7299351698160171, 0.7330253118276596, 0.7221178215742111, 0.7340238112211227, 0.7309358543157578, 0.7285109889507294, 0.727522132396698, 0.7252336782217026, 0.731070601940155, 0.7321604478359223, 0.7293566286563873, 0.7277899676561356, 0.7220651310682297, 0.7220740592479706, 0.7244965744018554, 0.7258755683898925, 0.7292886948585511, 0.7316807186603547, 0.733866173028946, 0.7328475558757782, 0.7270730751752853, 0.738879982829094, 0.7309868389368057, 0.7288133585453034, 0.735592747926712, 0.7320762103796006, 0.7227511954307556, 0.7282739782333374, 0.7351133835315704, 0.7261863315105438, 0.7215372592210769, 0.7215272682905197, 0.7158464950323105, 0.717667385339737, 0.7305703550577164, 0.7369882333278656, 0.72757757127285, 0.7256824326515198, 0.721240953207016, 0.7157500910758973, 0.7208325755596161, 0.7243972092866897, 0.7279701817035675, 0.7262749487161636, 0.7165784907341003, 0.7114552277326583, 0.7161766988039017, 0.7188788956403732, 0.7111408191919327, 0.7130656880140305, 0.7124124026298523, 0.7140775722265243, 0.7124007743597031, 0.7137226045131684, 0.7105222719907761, 0.7130525904893875, 0.709129586815834, 0.7171198862791062, 0.7224958604574203, 0.7292351168394089, 0.72635822057724, 0.7328797394037246, 0.7322849088907242, 0.7299927246570587, 0.7236864560842514, 0.7307378315925598, 0.7227314686775208, 0.7250301134586334, 0.7253345561027527, 0.7116204959154129, 0.7138955479860306, 0.7188591468334198, 0.7269688868522644, 0.736510534286499, 0.7440159213542938, 0.7380796468257904, 0.735493792295456, 0.7354804795980453, 0.7350157195329666, 0.7363791942596436, 0.7273197096586227, 0.7258964866399765, 0.726784570813179, 0.726415137052536, 0.7086425733566284, 0.7189440500736236, 0.714579605460167, 0.7198198819160462, 0.7130072551965714, 0.7233031624555588, 0.729365137219429, 0.7262634086608887, 0.721449773311615, 0.7143625962734222, 0.7143386852741241, 0.7123226928710937]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "!pip install progress\n",
    "!pip install tensorboardX\n",
    "!!pip install imgaug\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "#######Uncooment the following line to use \"wideresnet_28\" model\n",
    "import models.wideresnet_28 as models\n",
    "#######Uncooment the following line to use \"wideresnet_40\" model\n",
    "# import models.wideresnet_40 as models\n",
    "import dataset.cifar10 as dataset\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch MixMatch Training')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', default=120, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--batch-size', default=100, type=int, metavar='N',\n",
    "                    help='train batchsize')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "# Checkpoints\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "# Miscs\n",
    "parser.add_argument('--manualSeed', type=int, default=0, help='manual seed')\n",
    "#Device options\n",
    "parser.add_argument('--gpu', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "#Method options\n",
    "parser.add_argument('--n-labeled', type=int, default=500,\n",
    "                        help='Number of labeled data')\n",
    "parser.add_argument('--train-iteration', type=int, default=1024,\n",
    "                        help='Number of iteration per epoch')\n",
    "parser.add_argument('--out', default='result',\n",
    "                        help='Directory to output the result')\n",
    "parser.add_argument('--alpha', default=0.75, type=float)\n",
    "parser.add_argument('--lambda-u', default=75, type=float)\n",
    "parser.add_argument('--T', default=0.5, type=float)\n",
    "parser.add_argument('--ema-decay', default=0.999, type=float)\n",
    "\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "\n",
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "np.random.seed(args.manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "\n",
    "##################### This function is implemented by me ############################\n",
    "def sharpening(x, T):\n",
    "        temp = x**(1/T)\n",
    "        return temp / temp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "##################### This function is implemented by me ############################    \n",
    "def mixup(x1, x2, y1, y2, alpha):\n",
    "    beta = np.random.beta(alpha, alpha)\n",
    "    x = beta * x1 + (1 - beta) * x2\n",
    "    y = beta * y1 + (1 - beta) * y2\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def main():\n",
    "    global best_acc\n",
    "\n",
    "    if not os.path.isdir(args.out):\n",
    "        mkdir_p(args.out)\n",
    "\n",
    "    print(f'==> Preparing cifar10')\n",
    "    transform_train = transforms.Compose([\n",
    "        dataset.RandomPadandCrop(32),\n",
    "        dataset.RandomFlip(),\n",
    "        dataset.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "        dataset.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_labeled_set, train_unlabeled_set, val_set, test_set = dataset.get_cifar10('./data', args.n_labeled, transform_train=transform_train, transform_val=transform_val)\n",
    "    print(\"----------------------\")\n",
    "    print(type(train_labeled_set))\n",
    "    labeled_trainloader = data.DataLoader(train_labeled_set, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    unlabeled_trainloader = data.DataLoader(train_unlabeled_set, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "    val_loader = data.DataLoader(val_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    print(\"==> creating WRN-28\")\n",
    "    \n",
    "\n",
    "\n",
    "    def create_model(ema=False):\n",
    "        model = models.WideResNet(num_classes=10)\n",
    "        model = model.cuda()\n",
    "\n",
    "        if ema:\n",
    "            for param in model.parameters():\n",
    "                param.detach_()\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = create_model()\n",
    "    ema_model = create_model(ema=True)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "    train_criterion = SemiLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    ema_optimizer= WeightEMA(model, ema_model, alpha=args.ema_decay)\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume\n",
    "    if args.resume:\n",
    "        # Load checkpoint.\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        assert os.path.isfile(args.resume), 'Error: no checkpoint directory found!'\n",
    "        args.out = os.path.dirname(args.resume)\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        ema_model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logger = Logger(os.path.join(args.out, 'log.txt'), resume=True)\n",
    "    else:\n",
    "        logger = Logger(os.path.join(args.out, 'log.txt'))\n",
    "        logger.set_names(['Train Loss', 'Train Loss X', 'Train Loss U',  'Valid Loss', 'Valid Acc.', 'Test Loss', 'Test Acc.'])\n",
    "\n",
    "    writer = SummaryWriter(args.out)\n",
    "    step = 0\n",
    "    test_accs = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Train and val\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "        print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, state['lr']))\n",
    "\n",
    "        train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, train_criterion, epoch, use_cuda)\n",
    "        _, train_acc = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode='Train Stats')\n",
    "        val_loss, val_acc = validate(val_loader, ema_model, criterion, epoch, use_cuda, mode='Valid Stats')\n",
    "        test_loss, test_acc = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode='Test Stats ')\n",
    "\n",
    "        step = args.train_iteration * (epoch + 1)\n",
    "\n",
    "        writer.add_scalar('losses/train_loss', train_loss, step)\n",
    "        writer.add_scalar('losses/valid_loss', val_loss, step)\n",
    "        writer.add_scalar('losses/test_loss', test_loss, step)\n",
    "\n",
    "        writer.add_scalar('accuracy/train_acc', train_acc, step)\n",
    "        writer.add_scalar('accuracy/val_acc', val_acc, step)\n",
    "        writer.add_scalar('accuracy/test_acc', test_acc, step)\n",
    "\n",
    "        # append logger file\n",
    "        logger.append([train_loss, train_loss_x, train_loss_u, val_loss, val_acc, test_loss, test_acc])\n",
    "\n",
    "        # save model\n",
    "        is_best = val_acc > best_acc\n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'ema_state_dict': ema_model.state_dict(),\n",
    "                'acc': val_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best)\n",
    "        test_accs.append(test_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    logger.close()\n",
    "    writer.close()\n",
    "\n",
    "    print('Best acc:')\n",
    "    print(best_acc)\n",
    "\n",
    "    print('Mean acc:')\n",
    "    print(np.mean(test_accs[-20:]))\n",
    " \n",
    "    print(\"----------------Test loss--------------\")\n",
    "    print(test_losses)\n",
    "    print(\"----------------Validation loss--------\")\n",
    "    print(val_losses)\n",
    "\n",
    "\n",
    "def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch, use_cuda):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    ws = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    bar = Bar('Training', max=args.train_iteration)\n",
    "    labeled_train_iter = iter(labeled_trainloader)\n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx in range(args.train_iteration):\n",
    "        try:\n",
    "            inputs_x, targets_x = labeled_train_iter.next()\n",
    "        except:\n",
    "            labeled_train_iter = iter(labeled_trainloader)\n",
    "            inputs_x, targets_x = labeled_train_iter.next()\n",
    "\n",
    "        try:\n",
    "            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n",
    "        except:\n",
    "            unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        batch_size = inputs_x.size(0)\n",
    "\n",
    "        # Transform label to one-hot\n",
    "        targets_x = torch.zeros(batch_size, 10).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
    "            inputs_u = inputs_u.cuda()\n",
    "            inputs_u2 = inputs_u2.cuda()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute guessed labels of unlabel samples\n",
    "            outputs_u = model(inputs_u)\n",
    "            outputs_u2 = model(inputs_u2)\n",
    "            p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "            pt = sharpening(p,args.T)\n",
    "            targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "            targets_u = targets_u.detach()\n",
    "\n",
    "        \n",
    "        all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n",
    "        all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
    "\n",
    "        idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "        input_a, input_b = all_inputs, all_inputs[idx]\n",
    "        target_a, target_b = all_targets, all_targets[idx]\n",
    "        \n",
    "        #################### Called mixup function ################################\n",
    "        \n",
    "        mixed_input, mixed_target = mixup(input_a, input_b, target_a, target_b, args.alpha)\n",
    "\n",
    "        # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n",
    "        mixed_input = list(torch.split(mixed_input, batch_size))\n",
    "        mixed_input = interleave(mixed_input, batch_size)\n",
    "\n",
    "        logits = [model(mixed_input[0])]\n",
    "        for input in mixed_input[1:]:\n",
    "            logits.append(model(input))\n",
    "\n",
    "        # put interleaved samples back\n",
    "        logits = interleave(logits, batch_size)\n",
    "        logits_x = logits[0]\n",
    "        logits_u = torch.cat(logits[1:], dim=0)\n",
    "\n",
    "        Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_idx/args.train_iteration)\n",
    "\n",
    "        loss = Lx + w * Lu\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.item(), inputs_x.size(0))\n",
    "        losses_x.update(Lx.item(), inputs_x.size(0))\n",
    "        losses_u.update(Lu.item(), inputs_x.size(0))\n",
    "        ws.update(w, inputs_x.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f} | W: {w:.4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=args.train_iteration,\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    loss_x=losses_x.avg,\n",
    "                    loss_u=losses_u.avg,\n",
    "                    w=ws.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "\n",
    "    return (losses.avg, losses_x.avg, losses_u.avg,)\n",
    "\n",
    "def validate(valloader, model, criterion, epoch, use_cuda, mode):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar(f'{mode}', max=len(valloader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(valloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        )\n",
    "            bar.next()\n",
    "        bar.finish()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "\n",
    "######### The function implementation was taken from an internet resource for MixMatch implementation.\n",
    "def save_checkpoint(state, is_best, checkpoint=args.out, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "        \n",
    "        \n",
    "######### The function implementation was taken from an internet resource for MixMatch implementation.\n",
    "def linear_rampup(current, rampup_length=args.epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)\n",
    "\n",
    "######### The class implementation was taken from an internet resource for MixMatch implementation.\n",
    "class SemiLoss(object):\n",
    "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "\n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
    "        Lu = torch.mean((probs_u - targets_u)**2)\n",
    "\n",
    "        return Lx, Lu, args.lambda_u * linear_rampup(epoch)\n",
    "\n",
    "######### The class implementation was taken from an internet resource for MixMatch implementation.\n",
    "class WeightEMA(object):\n",
    "    def __init__(self, model, ema_model, alpha=0.999):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "        self.alpha = alpha\n",
    "        self.params = list(model.state_dict().values())\n",
    "        self.ema_params = list(ema_model.state_dict().values())\n",
    "        self.wd = 0.02 * args.lr\n",
    "\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            if ema_param.dtype==torch.float32:\n",
    "                ema_param.mul_(self.alpha)\n",
    "                ema_param.add_(param * one_minus_alpha)\n",
    "                # customized weight decay\n",
    "                param.mul_(1 - self.wd)\n",
    "                \n",
    "######### The function implementation was taken from an internet resource for MixMatch implementation.##############\n",
    "def interleave_offsets(batch, nu):\n",
    "    groups = [batch // (nu + 1)] * (nu + 1)\n",
    "    for x in range(batch - sum(groups)):\n",
    "        groups[-x - 1] += 1\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1] + g)\n",
    "    assert offsets[-1] == batch\n",
    "    return offsets\n",
    "\n",
    "######### The function implementation was taken from an internet resource for MixMatch implementation.##################\n",
    "def interleave(xy, batch):\n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch, nu)\n",
    "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
    "    for i in range(1, nu + 1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147c30a-296a-4910-adf7-81e775497134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
